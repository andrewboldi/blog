---
title: 'To Be or not to be Bayesian'
image: ''
alt: ''
created: 2025-07-07
tags:
    - 'Algorithmic Thinking'
---

Many people have said that Bayesian updating is a good framework for understanding how beliefs change over time. However, humans can significantly update their beliefs if they reach a set of logical conclusions. This is a significant driver in also updating one's beliefs. It's also a significant reason why humans are able to have such high reasoning capabilities with such limited data: we reason as a form of synthetic data generation. We use data to build a fundamental set of axioms. Then if we apply logical reasoning, we can create a vastly more significant better way of reasoning. And there is extreme value is boiling down axioms into more fundamental principles. This is how Elon Musk reasons. Rather than taking textbook knowledge as truth, he boils it down further into, say, Newton's laws or the Lagrangian/Hamiltonian formalisms. Then you go back to the advanced knowledge expressed in textbooks with your axiomatic thinking. And this gives you solid experience for new knowledge. This is a breakthrough if implemented. It's what Francois Chollet is working on.

Huge idea: basically we need to have an LLM continually be boiling down its entire corpus of generated knowledge into a vector DB of "axiomatic truths", "likely truths", or "false" statements. But we need to somehow update this in the model itself. We might be able to embed 
We essentially want to create circuits in a neural network based on truths. Subgraph of truths...


Very interesting that I came to this conclusion without seeing it from my initial thoughts of Bayesian thought. That literally proves my point. Francois Chollet's talk had such an impact (a single presentation) that it fundmanetally updated my belief. We have a "truth sensing detector". This should be built in. 
