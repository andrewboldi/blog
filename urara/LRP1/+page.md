---
title: 'So what is Layerwise Relevance Propagation (LRP)?'
image: ''
alt: ''
created: 2025-07-07
tags:
    - 'Machine Learning'
    - 'Interpretability'
    - 'Computer Science'
---

Let's say we have a simple neural network. How do we know what input neurons were most important for the final prediction? Discuss perturbation, etc, etc. Generally speaking a neural network consists of non-linear functions. So what can we do, approximate all the nonlinear functions as first-order Taylor expansions! (make a diagram) We also have some nice properties like conservation and positivity but they are not strictly required (and more recent papers ignore these constraints!). go into the derivation. Go into AttnLRP. Go into Chefer.

^ Actually it might be a good idea to have a review paper...?

